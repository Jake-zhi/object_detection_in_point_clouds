{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pixor_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavikngala/object_detection_in_point_clouds/blob/dev/pixor_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XMydEHYrX7Ab",
        "colab_type": "code",
        "outputId": "4b253ed3-8f1c-4799-ed58-728afa6b5342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jQXNlWvJFd9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "b62qPa3FEpMs",
        "colab_type": "code",
        "outputId": "61164fa2-1824-4f59-e7cf-d7782e357fff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --branch doogal https://<USERNAME>:<PASSWORD>@github.com/bhavikngala/object_detection_in_point_clouds.git\n",
        "!mv object_detection_in_point_clouds code\n",
        "%cd ./code\n",
        "!git pull origin doogal"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'object_detection_in_point_clouds'...\n",
            "remote: Enumerating objects: 135, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 1895 (delta 86), reused 76 (delta 36), pack-reused 1760\u001b[K\n",
            "Receiving objects: 100% (1895/1895), 6.10 MiB | 21.77 MiB/s, done.\n",
            "Resolving deltas: 100% (1258/1258), done.\n",
            "/content/code\n",
            "From https://github.com/bhavikngala/object_detection_in_point_clouds\n",
            " * branch            doogal     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ONQmYmc0Gu6m",
        "colab_type": "code",
        "outputId": "d86af973-9840-43be-dc2f-9de7ca3f58ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p models\n",
        "!mkdir -p loss\n",
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NLVCdHoHFYBS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Custom trainer"
      ]
    },
    {
      "metadata": {
        "id": "-nMEY64dFOXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import traceback\n",
        "import argparse\n",
        "import numpy as np\n",
        "try:\n",
        "  from tensorboardX import SummaryWriter\n",
        "  log = True\n",
        "except:\n",
        "  log = False\n",
        "\n",
        "import datautils.dataloader_v3 as dataloader\n",
        "import config as cnf\n",
        "import misc\n",
        "import networks.networks as networks\n",
        "import model_groomers as mg\n",
        "import lossUtils as lu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uBZALYIFz0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CustomGroomer(mg.ModelTrainer):\n",
        "  def __init__(self, log, logDir, modelFilename, **kwargs):\n",
        "    if log: self.writer = SummaryWriter(logDir)\n",
        "    self.iter = 0\n",
        "    self.logDir = logDir\n",
        "    self.modelFilename = modelFilename\n",
        "    self.clip_value = kwargs['clip_value']\n",
        "    self.accumulationSteps = kwargs['accumulationSteps']\n",
        "    self.lrRange2 = kwargs['lrRange2']\n",
        "    self.momentumRange2 = kwargs['momentumRange2']\n",
        "\n",
        "    \n",
        "  def train(self, device, log=False):\n",
        "    if self.loader is None:\n",
        "      print('data loader is undefined')\n",
        "      quit()\n",
        "      \n",
        "    self.model.train()\n",
        "\n",
        "    subBatchCounter = 0\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "      epochValues = []\n",
        "\n",
        "      self.scheduler.step()\n",
        "\n",
        "      batch_size = self.loader.batch_size\n",
        "      self.model.zero_grad()\n",
        "      for batchId, data in enumerate(self.loader):\n",
        "        lidar, targetClass, targetLoc, filenames = data\n",
        "\n",
        "        # ignore last batch\n",
        "        if lidar.size(0) != batch_size:\n",
        "          continue\n",
        "\n",
        "        lidar = lidar.cuda(device, non_blocking=True)\n",
        "        targetClass = [c.contiguous().cuda(device, non_blocking=True) for c in targetClass]\n",
        "        targetLoc = [loc.contiguous().cuda(device, non_blocking=True) for loc in targetLoc]\n",
        "\n",
        "        predictedClass, predictedLoc = self.model(lidar)\n",
        "\n",
        "        claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples \\\n",
        "         = self.lossFunction(predictedClass, predictedLoc, targetClass, targetLoc, self.alpha1, self.beta1)\n",
        "\n",
        "        if trainLoss is not None:\n",
        "          trainLoss = trainLoss/self.accumulationSteps\n",
        "          trainLoss.backward()\n",
        "          subBatchCounter += 1\n",
        "\n",
        "        if subBatchCounter==self.accumulationSteps:\n",
        "          self.optim.step()\n",
        "          self.model.zero_grad()\n",
        "          subBatchCounter = 0\n",
        "\n",
        "    \n",
        "        epochValues.append((claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples))\n",
        "      \n",
        "      self.epochLogger(epochValues, epoch, log)\n",
        "      self.saveModel(self.modelFilename)\n",
        "\n",
        "      \n",
        "  def val(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def epochLogger(self, epochValues, epoch, log=False):\n",
        "    pC, nC, locL, mC, mPT, nP, nN = 0, 0, 0, 0, 0, 0, 0\n",
        "    for i in range(len(epochValues)):\n",
        "      claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples = \\\n",
        "        epochValues[i]\n",
        "      if posClaLoss is not None:\n",
        "        pC = pC + posClaLoss*numPosSamples\n",
        "        locL = locL + locLoss*numPosSamples\n",
        "        mC = mC + meanConfidence*numPosSamples\n",
        "        nC = nC + negClaLoss*numNegSamples\n",
        "        mPT = mPT + overallMeanConfidence*(numPosSamples+numNegSamples)\n",
        "        nP += numPosSamples\n",
        "        nN += numNegSamples\n",
        "        \n",
        "    pC /= nP\n",
        "    nC /= nN\n",
        "    locL /= nP\n",
        "    mC /= nP\n",
        "    mPT /= (nP+nN)\n",
        "\n",
        "    '''\n",
        "    self.writer.add_scalar('train/epoch_classification_loss', pC+nC, epoch)\n",
        "    self.writer.add_scalar('train/epoch_pos_classification_loss', pC, epoch)\n",
        "    self.writer.add_scalar('train/epoch_neg_classification_loss', nC, epoch)\n",
        "    self.writer.add_scalar('train/epoch_localization_loss', locL, epoch)\n",
        "    self.writer.add_scalar('train/epoch_train_loss', pC+nC+locL, epoch)\n",
        "    self.writer.add_scalar('train/epoch_mean_pos_sample_confidence', mC, epoch)\n",
        "    self.writer.add_scalar('train/epoch_mean_pt', mPT, epoch)\n",
        "    '''\n",
        "    cl = pC+nC\n",
        "    tl = cl+locL\n",
        "    print(f'epoch: {epoch} | CL: {cl} | PC: {pC} | NC: {nC} | LL: {locL} | TL: {tl} | MPC: {mC} | MC: {mPT}')\n",
        "      \n",
        "\n",
        "  def setLoaders(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WG7ptbvsHwZ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "  # data loaders\n",
        "  trainLoader = DataLoader(\n",
        "    dataloader.KittiDataset(cnf, args, 'train'),\n",
        "    batch_size = cnf.batchSize, shuffle=True, num_workers=3,\n",
        "    collate_fn=dataloader.customCollateFunction, pin_memory=True\n",
        "  )\n",
        "\n",
        "  # define model\n",
        "  model = networks.PointCloudDetector2(\n",
        "    cnf.res_block_layers,\n",
        "    cnf.up_sample_layers,\n",
        "    cnf.deconv)\n",
        "\n",
        "  modelTrainer = CustomGroomer(False, cnf.logDir, args.model_file,\n",
        "    clip_value=cnf.clip_value,\n",
        "    accumulationSteps=cnf.accumulationSteps,\n",
        "    lrRange2 = cnf.lrRange2,\n",
        "    momentumRange2 = cnf.momentumRange2)\n",
        "  modelTrainer.setDataloader(trainLoader)\n",
        "  modelTrainer.setEpochs(cnf.epochs)\n",
        "  modelTrainer.setModel(model)\n",
        "  modelTrainer.setDataParallel(args.multi_gpu)\n",
        "  modelTrainer.copyModel(cnf.device)\n",
        "  modelTrainer.setOptimizer('sgd', cnf.slr, cnf.momentum, cnf.decay)\n",
        "  modelTrainer.setLRScheduler(cnf.lrDecay, cnf.milestones)\n",
        "\n",
        "  if False and os.path.isfile(args.model_file):\n",
        "    modelTrainer.loadModel(args.model_file)\n",
        "\n",
        "  modelTrainer.setLossFunction(lu.computeLoss, cnf.alpha1, cnf.beta1)\n",
        "  modelTrainer.train(cnf.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DG7d3cJpIhoM",
        "colab_type": "code",
        "outputId": "27cc7f2a-89a0-4eb6-a0f7-5bfb5f61122d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "MODEL_FILE = './models/p1.pth'\n",
        "cnf.rootDir = '/gdrive/My Drive/KITTI_BEV'\n",
        "cnf.calTrain = '/gdrive/My Drive/KITTI_BEV/training/calib'\n",
        "cnf.slr = 1e-2\n",
        "cnf.milestones = [20, 30]\n",
        "cnf.epochs = 40\n",
        "print(cnf.device)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Train network')\n",
        "parser.add_argument('-f', '--model-file', default=None, help='used to set different model file name other than default one')\n",
        "parser.add_argument('-m', '--multi-gpu', action='store_true', help='use multiple gpus?')\n",
        "parser.add_argument('--full-train-set', action='store_true', help='use entire training set or split into train val set')\n",
        "parser.add_argument('--plot-img', action='store_true', help='plot on img; used in validateNetwork.py')\n",
        "args = parser.parse_args()\n",
        "args.model_file = MODEL_FILE\n",
        "\n",
        "try:\n",
        "  main(args)\n",
        "except Exception as e:\n",
        "  with open('./error.txt', 'w') as f:\n",
        "    print(traceback.format_exc())\n",
        "    f.write(traceback.format_exc())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oMUSc6GRLjP0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tail ./../data/training/calib/000000.txt"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}