{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pixor_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavikngala/object_detection_in_point_clouds/blob/dev/pixor_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XMydEHYrX7Ab",
        "colab_type": "code",
        "outputId": "8512314b-c90e-4342-a8ea-4dfbbc96c40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jQXNlWvJFd9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "cSPYLYuyY-E6",
        "colab_type": "code",
        "outputId": "8eb1c96b-e079-47e2-cc9d-d0c0ee16af28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!mkdir -p data\n",
        "!cp -r /gdrive/My\\ Drive/KITTI_BEV/* ./data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "co-lYMOOa02H",
        "colab_type": "code",
        "outputId": "82415e45-dcae-4b64-8647-ba6cc587abcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!ls ./data/training/velodyne | wc -l\n",
        "!ls ./data/training/label_2 | wc -l\n",
        "!ls ./data/training/calib | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7481\n",
            "7481\n",
            "7482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b62qPa3FEpMs",
        "colab_type": "code",
        "outputId": "10ef9a17-6338-4196-8e9a-843609fc0f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --branch doogal https://<USERNAME>:<PASSWORD>@github.com/bhavikngala/object_detection_in_point_clouds.git\n",
        "!mv object_detection_in_point_clouds code\n",
        "%cd ./code\n",
        "!git pull origin doogal"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'object_detection_in_point_clouds'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 1877 (delta 73), reused 71 (delta 32), pack-reused 1760\u001b[K\n",
            "Receiving objects: 100% (1877/1877), 6.10 MiB | 21.60 MiB/s, done.\n",
            "Resolving deltas: 100% (1245/1245), done.\n",
            "/content/code\n",
            "From https://github.com/bhavikngala/object_detection_in_point_clouds\n",
            " * branch            doogal     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ONQmYmc0Gu6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf649cfb-84d9-479d-971e-2569b8dfb2f5"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p models\n",
        "!mkdir -p loss\n",
        "!pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NLVCdHoHFYBS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Custom trainer"
      ]
    },
    {
      "metadata": {
        "id": "-nMEY64dFOXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import traceback\n",
        "import argparse\n",
        "try:\n",
        "  from tensorboardX import SummaryWriter\n",
        "  log = True\n",
        "except:\n",
        "  log = False\n",
        "\n",
        "import datautils.dataloader_v3 as dataloader\n",
        "import config as cnf\n",
        "import misc\n",
        "import networks.networks as networks\n",
        "import model_groomers as mg\n",
        "import lossUtils as lu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uBZALYIFz0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CustomGroomer(mg.ModelTrainer):\n",
        "\n",
        "  def __init__(self, log, logDir, modelFilename, **kwargs):\n",
        "    if log: self.writer = SummaryWriter(logDir)\n",
        "    self.iter = 0\n",
        "    self.logDir = logDir\n",
        "    self.modelFilename = modelFilename\n",
        "    self.clip_value = kwargs['clip_value']\n",
        "    self.accumulationSteps = kwargs['accumulationSteps']\n",
        "    self.lrRange2 = kwargs['lrRange2']\n",
        "    self.momentumRange2 = kwargs['momentumRange2']\n",
        "\n",
        "  def train(self, device, log=False):\n",
        "    if self.loader is None:\n",
        "      print('data loader is undefined')\n",
        "      quit()\n",
        "\n",
        "    subBatchCounter = 0\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "      epochValues = []\n",
        "\n",
        "      if self.oneCycleLearning:\n",
        "        if epoch > self.stepSize*2:\n",
        "          self.setLrRangeStepSize(self.lrRange2, self.momentumRange2, self.stepSize)\n",
        "        self.oneCycleStep(epoch)\n",
        "      else:\n",
        "        self.scheduler.step()\n",
        "\n",
        "      batch_size = self.loader.batch_size\n",
        "      self.model.zero_grad()\n",
        "      for batchId, data in enumerate(self.loader):\n",
        "        lidar, targetClass, targetLoc, filenames = data\n",
        "\n",
        "        # ignore last batch\n",
        "        if lidar.size(0) != batch_size:\n",
        "          continue\n",
        "\n",
        "        lidar = lidar.cuda(device, non_blocking=True)\n",
        "        targetClass = [c.contiguous().cuda(device, non_blocking=True) for c in targetClass]\n",
        "        targetLoc = [loc.contiguous().cuda(device, non_blocking=True) for loc in targetLoc]\n",
        "\n",
        "        predictedClass, predictedLoc = self.model(lidar)\n",
        "\n",
        "        claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples \\\n",
        "         = self.lossFunction(predictedClass, predictedLoc, targetClass, targetLoc, self.alpha1, self.beta1)\n",
        "\n",
        "        if trainLoss is not None:\n",
        "          trainLoss = trainLoss/self.accumulationSteps\n",
        "          trainLoss.backward()\n",
        "          subBatchCounter += 1\n",
        "\n",
        "        if subBatchCounter==self.accumulationSteps:\n",
        "          # if self.clip_value:\n",
        "          #  torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_value)\n",
        "          self.optim.step()\n",
        "          self.model.zero_grad()\n",
        "          subBatchCounter = 0\n",
        "\n",
        "    \n",
        "        epochValues.append((claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples))\n",
        "        if log: self.iterLogger((claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples))\n",
        "          \n",
        "      self.epochLogger(epochValues, epoch, log)\n",
        "      self.saveModel(self.modelFilename)\n",
        "\n",
        "  def val(self):\n",
        "    pass\n",
        "\n",
        "  def iterLogger(self, values):\n",
        "    claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples = \\\n",
        "      values\n",
        "    self.writer.add_scalar('data/classification_loss', claLoss, self.iter)\n",
        "    if posClaLoss is not None:\n",
        "      self.writer.add_scalar('data/pos_classification_loss', posClaLoss, self.iter)\n",
        "      self.writer.add_scalar('data/localization_loss', locLoss, self.iter)\n",
        "      self.writer.add_scalar('data/mean_pos_sample_confidence', meanConfidence, self.iter)\n",
        "    self.writer.add_scalar('data/neg_classification_loss', negClaLoss, self.iter)\n",
        "    self.writer.add_scalar('data/train_loss', trainLoss, self.iter)\n",
        "    self.writer.add_scalar('data/mean_pt', overallMeanConfidence, self.iter)\n",
        "    self.writer.add_scalar('data/pos_samples', numPosSamples, self.iter)\n",
        "    self.writer.add_scalar('data/neg_samples', numNegSamples, self.iter)\n",
        "    self.iter += 1\n",
        "\n",
        "  def epochLogger(self, epochValues, epoch, log=False):\n",
        "    pC, nC, locL, mC, mPT, nP, nN = 0, 0, 0, 0, 0, 0, 0\n",
        "    for i in range(len(epochValues)):\n",
        "      claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples = \\\n",
        "        epochValues[i]\n",
        "      if posClaLoss is not None:\n",
        "        pC = pC + posClaLoss*numPosSamples\n",
        "        locL = locL + locLoss*numPosSamples\n",
        "        mC = mC + meanConfidence*numPosSamples\n",
        "      nC = nC + negClaLoss*numNegSamples\n",
        "      mPT = mPT + overallMeanConfidence*(numPosSamples+numNegSamples)\n",
        "      nP += numPosSamples\n",
        "      nN += numNegSamples\n",
        "\n",
        "    pC /= nP\n",
        "    nC /= nN\n",
        "    locL /= nP\n",
        "    mC /= nP\n",
        "    mPT /= (nP+nN)\n",
        "\n",
        "    if log:\n",
        "      self.writer.add_scalar('train/epoch_classification_loss', pC+nC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_pos_classification_loss', pC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_neg_classification_loss', nC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_localization_loss', locL, epoch)\n",
        "      self.writer.add_scalar('train/epoch_train_loss', pC+nC+locL, epoch)\n",
        "      self.writer.add_scalar('train/epoch_mean_pos_sample_confidence', mC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_mean_pt', mPT, epoch)\n",
        "    else:\n",
        "      cl = pC + nC\n",
        "      tl = pC+nC+locL\n",
        "      print(f'epoch: {epoch} | CL: {cl} | PC: {pC} | NC: {nC} | LL: {locL} | TL: {tl} | MPC: {mC} | MC: {mPT}')\n",
        "\n",
        "  def exportLogs(self, filename):\n",
        "    # export scalar data to JSON for external processing\n",
        "    self.writer.export_scalars_to_json(filename)\n",
        "    self.writer.close()\n",
        "\n",
        "  def setLoaders(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WG7ptbvsHwZ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "  # data loaders\n",
        "  trainLoader = DataLoader(\n",
        "    dataloader.KittiDataset(cnf, args, 'train'),\n",
        "    batch_size = cnf.batchSize, shuffle=True, num_workers=3,\n",
        "    collate_fn=dataloader.customCollateFunction, pin_memory=True\n",
        "  )\n",
        "\n",
        "  # define model\n",
        "  model = networks.PointCloudDetector2(\n",
        "    cnf.res_block_layers,\n",
        "    cnf.up_sample_layers,\n",
        "    cnf.deconv)\n",
        "\n",
        "  modelTrainer = CustomGroomer(False, cnf.logDir, args.model_file,\n",
        "    clip_value=cnf.clip_value,\n",
        "    accumulationSteps=cnf.accumulationSteps,\n",
        "    lrRange2 = cnf.lrRange2,\n",
        "    momentumRange2 = cnf.momentumRange2)\n",
        "  modelTrainer.setDataloader(trainLoader)\n",
        "  modelTrainer.setEpochs(cnf.epochs)\n",
        "  modelTrainer.setModel(model)\n",
        "  modelTrainer.setDataParallel(args.multi_gpu)\n",
        "  modelTrainer.copyModel(cnf.device)\n",
        "  modelTrainer.setOptimizer('sgd', cnf.slr, cnf.momentum, cnf.decay)\n",
        "  if cnf.cycleLearn:\n",
        "    modelTrainer.setLrRangeStepSize(cnf.lrRange, cnf.momentumRange, cnf.stepSize)\n",
        "  else:\n",
        "    modelTrainer.setLRScheduler(cnf.lrDecay, cnf.milestones)\n",
        "\n",
        "  if False and os.path.isfile(args.model_file):\n",
        "    modelTrainer.loadModel(args.model_file)\n",
        "\n",
        "  modelTrainer.setLossFunction(lu.computeLoss, cnf.alpha1, cnf.beta1)\n",
        "  modelTrainer.train(cnf.device)\n",
        "  modelTrainer.exportLogs(cnf.logJSONFilename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DG7d3cJpIhoM",
        "colab_type": "code",
        "outputId": "7485d763-1f70-4e47-a3db-83b29ad16743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1193
        }
      },
      "cell_type": "code",
      "source": [
        "MODEL_FILE = './models/p1.pth'\n",
        "cnf.rootDir = '/gdrive/My Drive/KITTI_BEV'\n",
        "cnf.calTrain = '/gdrive/My Drive/KITTI_BEV/training/calib'\n",
        "print(cnf.device)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Train network')\n",
        "parser.add_argument('-f', '--model-file', default=None, help='used to set different model file name other than default one')\n",
        "parser.add_argument('-m', '--multi-gpu', action='store_true', help='use multiple gpus?')\n",
        "parser.add_argument('--full-train-set', action='store_true', help='use entire training set or split into train val set')\n",
        "parser.add_argument('--plot-img', action='store_true', help='plot on img; used in validateNetwork.py')\n",
        "args = parser.parse_args()\n",
        "args.model_file = MODEL_FILE\n",
        "\n",
        "try:\n",
        "  main(args)\n",
        "except Exception as e:\n",
        "  with open('./error.txt', 'w') as f:\n",
        "    print(traceback.format_exc())\n",
        "    f.write(traceback.format_exc())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "epoch: 1 | CL: 0.14717775583267212 | PC: 0.14717775583267212 | NC: 0.0 | LL: nan | TL: nan | MPC: 0.963475143523699 | MC: 0.9999679980330134\n",
            "epoch: 2 | CL: 0.0 | PC: 0.0 | NC: 0.0 | LL: nan | TL: nan | MPC: 1.0 | MC: 1.0\n",
            "epoch: 3 | CL: 0.0 | PC: 0.0 | NC: 0.0 | LL: nan | TL: nan | MPC: 1.0 | MC: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-2d79dce42ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./error.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-3a0825f00e72>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mmodelTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mmodelTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mmodelTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexportLogs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogJSONFilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-5ddead698942>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, device, log)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mpredictedClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictedLoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlidar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mclaLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposClaLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegClaLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanConfidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverallMeanConfidence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPosSamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumNegSamples\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictedClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictedLoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetLoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainLoss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/code/lossUtils.py\u001b[0m in \u001b[0;36mcomputeLoss7\u001b[0;34m(cla, loc, targetClas, targetLocs, alpha, beta, claLossOnly)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtargetCla\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mnumTargetsInFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mnumPosSamples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnumTargetsInFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oMUSc6GRLjP0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tail ./../data/training/calib/000000.txt"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}