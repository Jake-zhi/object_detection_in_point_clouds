{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pixor_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavikngala/object_detection_in_point_clouds/blob/dev/pixor_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jQXNlWvJFd9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "XMydEHYrX7Ab",
        "colab_type": "code",
        "outputId": "e46f420d-b770-4be4-b46b-5bc4e857bdb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cSPYLYuyY-E6",
        "colab_type": "code",
        "outputId": "8eb1c96b-e079-47e2-cc9d-d0c0ee16af28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!mkdir -p data\n",
        "!cp -r /gdrive/My\\ Drive/KITTI_BEV/* ./data\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "co-lYMOOa02H",
        "colab_type": "code",
        "outputId": "82415e45-dcae-4b64-8647-ba6cc587abcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!ls ./data/training/velodyne | wc -l\n",
        "!ls ./data/training/label_2 | wc -l\n",
        "!ls ./data/training/calib | wc -l"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7481\n",
            "7481\n",
            "7482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b62qPa3FEpMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "2ea11b20-49e2-4c93-bb8a-e0493970c3ae"
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone --branch doogal https://<USERNAME>:<PASSWORD>@github.com/bhavikngala/object_detection_in_point_clouds.git\n",
        "!mv object_detection_in_point_clouds code\n",
        "%cd ./code\n",
        "!git pull origin doogal"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'object_detection_in_point_clouds'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 1869 (delta 67), reused 69 (delta 31), pack-reused 1760\u001b[K\n",
            "Receiving objects: 100% (1869/1869), 6.09 MiB | 27.48 MiB/s, done.\n",
            "Resolving deltas: 100% (1239/1239), done.\n",
            "/content/code\n",
            "From https://github.com/bhavikngala/object_detection_in_point_clouds\n",
            " * branch            doogal     -> FETCH_HEAD\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "orr2ReRqKk3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "45363bf8-70ad-4633-86d4-4e65affadebd"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%cd ./code"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ONQmYmc0Gu6m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p models\n",
        "!mkdir -p loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLVCdHoHFYBS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Custom trainer"
      ]
    },
    {
      "metadata": {
        "id": "-nMEY64dFOXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "import traceback\n",
        "import argparse\n",
        "try:\n",
        "  from tensorboardX import SummaryWriter\n",
        "  log = True\n",
        "except:\n",
        "  log = False\n",
        "\n",
        "import datautils.dataloader_v3 as dataloader\n",
        "import config as cnf\n",
        "import misc\n",
        "import networks.networks as networks\n",
        "import model_groomers as mg\n",
        "import lossUtils as lu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0uBZALYIFz0a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CustomGroomer(mg.ModelTrainer):\n",
        "\n",
        "  def __init__(self, log, logDir, modelFilename, **kwargs):\n",
        "    if log: self.writer = SummaryWriter(logDir)\n",
        "    self.iter = 0\n",
        "    self.logDir = logDir\n",
        "    self.modelFilename = modelFilename\n",
        "    self.clip_value = kwargs['clip_value']\n",
        "    self.accumulationSteps = kwargs['accumulationSteps']\n",
        "    self.lrRange2 = kwargs['lrRange2']\n",
        "    self.momentumRange2 = kwargs['momentumRange2']\n",
        "\n",
        "  def train(self, device, log=False):\n",
        "    if self.loader is None:\n",
        "      print('data loader is undefined')\n",
        "      quit()\n",
        "\n",
        "    subBatchCounter = 0\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "      epochValues = []\n",
        "\n",
        "      if self.oneCycleLearning:\n",
        "        if epoch > self.stepSize*2:\n",
        "          self.setLrRangeStepSize(self.lrRange2, self.momentumRange2, self.stepSize)\n",
        "        self.oneCycleStep(epoch)\n",
        "      else:\n",
        "        self.scheduler.step()\n",
        "\n",
        "      batch_size = self.loader.batch_size\n",
        "      self.model.zero_grad()\n",
        "      for batchId, data in enumerate(self.loader):\n",
        "        lidar, targetClass, targetLoc, filenames = data\n",
        "\n",
        "        # ignore last batch\n",
        "        if lidar.size(0) != batch_size:\n",
        "          continue\n",
        "\n",
        "        lidar = lidar.cuda(device, non_blocking=True)\n",
        "        targetClass = [c.contiguous().cuda(device, non_blocking=True) for c in targetClass]\n",
        "        targetLoc = [loc.contiguous().cuda(device, non_blocking=True) for loc in targetLoc]\n",
        "\n",
        "        predictedClass, predictedLoc = self.model(lidar)\n",
        "\n",
        "        claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples \\\n",
        "         = self.lossFunction(predictedClass, predictedLoc, targetClass, targetLoc, self.alpa1, self.beta1)\n",
        "\n",
        "        if trainLoss is not None:\n",
        "          trainLoss = trainLoss/self.accumulationSteps\n",
        "          trainLoss.backward()\n",
        "          subBatchCounter += 1\n",
        "\n",
        "        if subBatchCounter==self.accumulationSteps:\n",
        "          if self.clip_value:\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_value)\n",
        "          self.optim.step()\n",
        "          self.model.zero_grad()\n",
        "          subBatchCounter = 0\n",
        "\n",
        "    \n",
        "        epochValues.append((claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples))\n",
        "        if log: self.iterLogger((claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples))\n",
        "          \n",
        "      self.epochLogger(epochValues, epoch, log)\n",
        "      self.saveModel(self.modelFilename)\n",
        "\n",
        "  def val(self):\n",
        "    pass\n",
        "\n",
        "  def iterLogger(self, values):\n",
        "    claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples = \\\n",
        "      values\n",
        "    self.writer.add_scalar('data/classification_loss', claLoss, self.iter)\n",
        "    if posClaLoss is not None:\n",
        "      self.writer.add_scalar('data/pos_classification_loss', posClaLoss, self.iter)\n",
        "      self.writer.add_scalar('data/localization_loss', locLoss, self.iter)\n",
        "      self.writer.add_scalar('data/mean_pos_sample_confidence', meanConfidence, self.iter)\n",
        "    self.writer.add_scalar('data/neg_classification_loss', negClaLoss, self.iter)\n",
        "    self.writer.add_scalar('data/train_loss', trainLoss, self.iter)\n",
        "    self.writer.add_scalar('data/mean_pt', overallMeanConfidence, self.iter)\n",
        "    self.writer.add_scalar('data/pos_samples', numPosSamples, self.iter)\n",
        "    self.writer.add_scalar('data/neg_samples', numNegSamples, self.iter)\n",
        "    self.iter += 1\n",
        "\n",
        "  def epochLogger(self, epochValues, epoch, log=False):\n",
        "    pC, nC, locL, mC, mPT, nP, nN = 0, 0, 0, 0, 0, 0, 0\n",
        "    for i in range(len(epochValues)):\n",
        "      claLoss, locLoss, trainLoss, posClaLoss, negClaLoss, meanConfidence, overallMeanConfidence, numPosSamples, numNegSamples = \\\n",
        "        epochValues[i]\n",
        "      if posClaLoss is not None:\n",
        "        pC = pC + posClaLoss*numPosSamples\n",
        "        locL = locL + locLoss*numPosSamples\n",
        "        mC = mC + meanConfidence*numPosSamples\n",
        "      nC = nC + negClaLoss*numNegSamples\n",
        "      mPT = mPT + overallMeanConfidence*(numPosSamples+numNegSamples)\n",
        "      nP += numPosSamples\n",
        "      nN += numNegSamples\n",
        "\n",
        "    pC /= nP\n",
        "    nC /= nN\n",
        "    locL /= nP\n",
        "    mC /= nP\n",
        "    mPT /= (nP+nN)\n",
        "\n",
        "    if log:\n",
        "      self.writer.add_scalar('train/epoch_classification_loss', pC+nC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_pos_classification_loss', pC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_neg_classification_loss', nC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_localization_loss', locL, epoch)\n",
        "      self.writer.add_scalar('train/epoch_train_loss', pC+nC+locL, epoch)\n",
        "      self.writer.add_scalar('train/epoch_mean_pos_sample_confidence', mC, epoch)\n",
        "      self.writer.add_scalar('train/epoch_mean_pt', mPT, epoch)\n",
        "    else:\n",
        "      cl = pC + nC\n",
        "      tl = pC+nC+locL\n",
        "      print(f'epoch: {epoch} | CL: {cl} | PC: {pC} | NC: {nC} | LL: {locL} | TL: {tl} | MPC: {mC} | MC: {mPT}')\n",
        "\n",
        "  def exportLogs(self, filename):\n",
        "    # export scalar data to JSON for external processing\n",
        "    self.writer.export_scalars_to_json(filename)\n",
        "    self.writer.close()\n",
        "\n",
        "  def setLoaders(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WG7ptbvsHwZ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "  # data loaders\n",
        "  trainLoader = DataLoader(\n",
        "    dataloader.KittiDataset(cnf, args, 'train'),\n",
        "    batch_size = cnf.batchSize, shuffle=True, num_workers=3,\n",
        "    collate_fn=dataloader.customCollateFunction, pin_memory=True\n",
        "  )\n",
        "\n",
        "  # define model\n",
        "  model = networks.PointCloudDetector2(\n",
        "    cnf.res_block_layers,\n",
        "    cnf.up_sample_layers,\n",
        "    cnf.deconv)\n",
        "\n",
        "  modelTrainer = CustomGroomer(False, cnf.logDir, args.model_file,\n",
        "    clip_value=cnf.clip_value,\n",
        "    accumulationSteps=cnf.accumulationSteps,\n",
        "    lrRange2 = cnf.lrRange2,\n",
        "    momentumRange2 = cnf.momentumRange2)\n",
        "  modelTrainer.setDataloader(trainLoader)\n",
        "  modelTrainer.setEpochs(cnf.epochs)\n",
        "  modelTrainer.setModel(model)\n",
        "  modelTrainer.setDataParallel(args.multi_gpu)\n",
        "  modelTrainer.copyModel(cnf.device)\n",
        "  modelTrainer.setOptimizer('sgd', cnf.slr, cnf.momentum, cnf.decay)\n",
        "  if cnf.cycleLearn:\n",
        "    modelTrainer.setLrRangeStepSize(cnf.lrRange, cnf.momentumRange, cnf.stepSize)\n",
        "  else:\n",
        "    modelTrainer.setLRScheduler(cnf.lrDecay, cnf.milestones)\n",
        "\n",
        "  if os.path.isfile(args.model_file):\n",
        "    modelTrainer.loadModel(args.model_file)\n",
        "\n",
        "  modelTrainer.setLossFunction(lu.computeLoss, cnf.alpha1, cnf.beta1)\n",
        "  modelTrainer.train(cnf.device, cnf.pretrainCla)\n",
        "  modelTrainer.exportLogs(cnf.logJSONFilename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DG7d3cJpIhoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "b0218ba7-7b3d-4243-f323-c3ad4bebf708"
      },
      "cell_type": "code",
      "source": [
        "MODEL_FILE = './models/p1.pth'\n",
        "cnf.rootDir = './../data'\n",
        "cnf.calTrain = './../data/training/calib'\n",
        "print(cnf.device)\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Train network')\n",
        "parser.add_argument('-f', '--model-file', default=None, help='used to set different model file name other than default one')\n",
        "parser.add_argument('-m', '--multi-gpu', action='store_true', help='use multiple gpus?')\n",
        "parser.add_argument('--full-train-set', action='store_true', help='use entire training set or split into train val set')\n",
        "parser.add_argument('--plot-img', action='store_true', help='plot on img; used in validateNetwork.py')\n",
        "args = parser.parse_args()\n",
        "args.model_file = MODEL_FILE\n",
        "\n",
        "try:\n",
        "  main(args)\n",
        "except Exception as e:\n",
        "  with open('./error.txt', 'w') as f:\n",
        "    print(traceback.format_exc())\n",
        "    f.write(traceback.format_exc())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-11-24559fb0120c>\", line 15, in <module>\n",
            "    main(args)\n",
            "  File \"<ipython-input-10-5e74f4a9afca>\", line 35, in main\n",
            "    modelTrainer.train(cnf.device, cnf.pretrainCla)\n",
            "  File \"<ipython-input-6-95c4563b181e>\", line 31, in train\n",
            "    for batchId, data in enumerate(self.loader):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 637, in __next__\n",
            "    return self._process_next_batch(batch)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 658, in _process_next_batch\n",
            "    raise batch.exc_type(batch.exc_msg)\n",
            "KeyError: 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\\n    samples = collate_fn([dataset[i] for i in batch_indices])\\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\\n    samples = collate_fn([dataset[i] for i in batch_indices])\\n  File \"/content/code/datautils/dataloader_v3.py\", line 71, in __getitem__\\n    self.projectionObject.setCalibrationMatrices(calibDict)\\n  File \"/content/code/datautils/kitti_utils.py\", line 67, in setCalibrationMatrices\\n    self.P = calibDict[\\'P2\\'].reshape([3,4])\\nKeyError: \\'P2\\'\\n'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oMUSc6GRLjP0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tail ./../data/training/calib/000000.txt"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}